{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anthropic version: 0.37.1\n",
      "🔧 Environment setup complete\n",
      "📁 Input directory: /Users/kemi/Documents/GitHub/vocab/src/content/articles\n",
      "📁 Output directory: /Users/kemi/Documents/GitHub/vocab/src/components/articles\n",
      "📊 Token tracking enabled\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "# All necessary imports and API key setup\n",
    "import os\n",
    "import yaml\n",
    "import anthropic\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from lucideIconList import icons\n",
    "\n",
    "print(f\"Anthropic version: {anthropic.__version__}\")\n",
    "\n",
    "# API Key Setup\n",
    "os.environ['ANTHROPIC_API_KEY'] = \"your-api-here\"  # Replace with your actual API key\n",
    "\n",
    "# Directory paths\n",
    "INPUT_DIR = \"/Users/kemi/Documents/GitHub/vocab/src/content/articles\"\n",
    "OUTPUT_DIR = \"/Users/kemi/Documents/GitHub/vocab/src/components/articles\"\n",
    "\n",
    "# Constants for Claude API\n",
    "MODEL_NAME = \"claude-3-5-sonnet-20241022\"\n",
    "MAX_TOKENS = 6000\n",
    "TEMPERATURE = 0.7\n",
    "\n",
    "# Token tracking constants\n",
    "TOKEN_LOG_FILE = \"token_usage_log.json\"\n",
    "TOKEN_SUMMARY_FILE = \"token_usage_summary.json\"\n",
    "\n",
    "LUCIDEICONS = icons\n",
    "\n",
    "class TokenUsage:\n",
    "    def __init__(self, input_tokens: int = 0, output_tokens: int = 0):\n",
    "        self.input_tokens = input_tokens\n",
    "        self.output_tokens = output_tokens\n",
    "        self.total_tokens = input_tokens + output_tokens\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            'input_tokens': self.input_tokens,\n",
    "            'output_tokens': self.output_tokens,\n",
    "            'total_tokens': self.total_tokens\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_response(cls, response) -> 'TokenUsage':\n",
    "        return cls(\n",
    "            input_tokens=response.usage.input_tokens,\n",
    "            output_tokens=response.usage.output_tokens\n",
    "        )\n",
    "\n",
    "def log_token_usage(component_name: str, shot1_usage: TokenUsage, shot2_usage: TokenUsage):\n",
    "    \"\"\"Log token usage for a component generation to JSON file\"\"\"\n",
    "    log_entry = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'component': component_name,\n",
    "        'shot1': shot1_usage.to_dict(),\n",
    "        'shot2': shot2_usage.to_dict(),\n",
    "        'total': {\n",
    "            'input_tokens': shot1_usage.input_tokens + shot2_usage.input_tokens,\n",
    "            'output_tokens': shot1_usage.output_tokens + shot2_usage.output_tokens,\n",
    "            'total_tokens': shot1_usage.total_tokens + shot2_usage.total_tokens\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Load existing log\n",
    "    try:\n",
    "        with open(TOKEN_LOG_FILE, 'r') as f:\n",
    "            log = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        log = []\n",
    "    \n",
    "    # Append new entry\n",
    "    log.append(log_entry)\n",
    "    \n",
    "    # Save updated log\n",
    "    with open(TOKEN_LOG_FILE, 'w') as f:\n",
    "        json.dump(log, f, indent=2)\n",
    "\n",
    "def update_token_summary():\n",
    "    \"\"\"Update the token usage summary file\"\"\"\n",
    "    try:\n",
    "        with open(TOKEN_LOG_FILE, 'r') as f:\n",
    "            log = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return\n",
    "    \n",
    "    summary = {\n",
    "        'total_components': len(log),\n",
    "        'total_tokens': sum(entry['total']['total_tokens'] for entry in log),\n",
    "        'average_tokens_per_component': sum(entry['total']['total_tokens'] for entry in log) / len(log),\n",
    "        'shot1_average': sum(entry['shot1']['total_tokens'] for entry in log) / len(log),\n",
    "        'shot2_average': sum(entry['shot2']['total_tokens'] for entry in log) / len(log),\n",
    "        'last_updated': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open(TOKEN_SUMMARY_FILE, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "# Print setup confirmation\n",
    "print(\"🔧 Environment setup complete\")\n",
    "print(f\"📁 Input directory: {INPUT_DIR}\")\n",
    "print(f\"📁 Output directory: {OUTPUT_DIR}\")\n",
    "print(\"📊 Token tracking enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Base functions loaded\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 - Base Functions\n",
    "\n",
    "def create_concept_prompt(title: str, summary: str) -> str:\n",
    "    \"\"\"\n",
    "    Creates the first-shot prompt focusing on conceptual understanding and metaphors.\n",
    "    \n",
    "    Args:\n",
    "        title (str): The concept title\n",
    "        summary (str): Brief summary of the concept\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted conceptual prompt\n",
    "    \"\"\"\n",
    "    prompt = f'''\n",
    "Explain the concept of {title} through various metaphors, real-world examples and an idea for a react component.\n",
    "Do not write any code yet.\n",
    "\n",
    "CONCEPT BREAKDOWN:\n",
    "1. Name of the concept: {title}\n",
    "2. Core Principle: {summary}\n",
    "\n",
    "APPROACH:\n",
    "- Choose 2 or 3 central, relatable metaphors or real-world examples that captures the essence of {title}\n",
    "- Example: For \"Neural Networks\" → use a \"Learning to Ride a Bike\" metaphor, \"Learning to Cook\" metaphor or \"Learning a New Language\" metaphor.\n",
    "- Create new metaphor examples\n",
    "\n",
    "VALIDATION:\n",
    "- How does each metaphor or real-world example map to technical concepts?\n",
    "- What misconceptions might it create?\n",
    "- How can we verify student understanding?\n",
    "\n",
    "LEARNING OBJECTIVES:\n",
    "- What should students be able to explain after?\n",
    "- What skills should they demonstrate?\n",
    "- How can we check for understanding?\n",
    "'''\n",
    "    return prompt\n",
    "\n",
    "def create_implementation_prompt(title: str, summary: str, concept_response: str) -> str:\n",
    "    \"\"\"\n",
    "    Creates the second-shot prompt for component implementation.\n",
    "    \n",
    "    Args:\n",
    "        title (str): The concept title\n",
    "        summary (str): Brief summary of the concept\n",
    "        concept_response (str): Claude's response from the first shot\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted implementation prompt\n",
    "    \"\"\"\n",
    "    prompt = f'''\n",
    "Create an intuitive React component that teaches {title} to 15 to 18-year-old human students.\n",
    "\n",
    "CONCEPT BREAKDOWN:\n",
    "1. Name of the concept: {title}\n",
    "2. Core Principle: {summary}\n",
    "3. Claude's explanation and idea: {concept_response}\n",
    "\n",
    "EDUCATIONAL GOALS:\n",
    "1. Help the human student understand {title} by connecting it to familiar concepts\n",
    "2. Show practical applications they might encounter in daily life\n",
    "3. Build understanding through progressive revelation, not all at once\n",
    "\n",
    "VISUALIZATION APPROACH:\n",
    "1. Animation & Interaction:\n",
    "   - Start with an automatic demo cycle using useEffect with proper cleanup\n",
    "   - Implement timing logic using useEffect hook with cleanup functions\n",
    "   - Allow human user interaction when relevant\n",
    "   - The interactions might include, but not limited to, moving objects, selecting objects given criteria, \n",
    "     sliding objects, Scroll or Pinch-to-zoom, Swipe navigation, Drag and Drop Operations, \n",
    "     Scroll-Based Interactions, Interactive tutorials\n",
    "   - Avoid using just a \"next/start/pause/play\" button as interaction\n",
    "   - Provide reset capability\n",
    "   - Use humor and relatable situations when appropriate\n",
    "   - Show cause-and-effect clearly\n",
    "\n",
    "2. Visual Elements:\n",
    "   - Every visual element should map to a real concept\n",
    "   - Use Lucide icons as meaningful symbols, not decoration\n",
    "   - You can write short texts to help convey the concept (no more than 25 words)\n",
    "\n",
    "3. Accessibility Requirements:\n",
    "   - Add ARIA roles and attributes where native elements don't cover the interaction\n",
    "   - Ensure keyboard navigation support\n",
    "   - Proper focus management\n",
    "   - Adequate color contrast (4.5:1 for regular text, 3:1 for large text)\n",
    "   - Don't rely solely on color to convey information\n",
    "\n",
    "Remember: A successful visualization is one where users can explain the concept to others after using it.\n",
    "'''\n",
    "    return prompt\n",
    "\n",
    "def extract_frontmatter(content: str) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Extract YAML frontmatter from markdown content.\n",
    "    \n",
    "    Args:\n",
    "        content (str): Full markdown file content\n",
    "        \n",
    "    Returns:\n",
    "        dict or None: Extracted frontmatter as dictionary, None if extraction fails\n",
    "    \"\"\"\n",
    "    if content.startswith('---'):\n",
    "        parts = content.split('---', 2)[1:]\n",
    "        if len(parts) >= 1:\n",
    "            try:\n",
    "                return yaml.safe_load(parts[0])\n",
    "            except yaml.YAMLError as e:\n",
    "                print(f\"  ⚠️ Error parsing YAML frontmatter: {str(e)}\")\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "def format_time(seconds: float) -> str:\n",
    "    \"\"\"\n",
    "    Format seconds into minutes and seconds.\n",
    "    \n",
    "    Args:\n",
    "        seconds (float): Number of seconds\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted string like \"1m 30s\"\n",
    "    \"\"\"\n",
    "    return f\"{int(seconds // 60)}m {int(seconds % 60)}s\"\n",
    "\n",
    "def get_existing_component_names(output_dir: str) -> set:\n",
    "    \"\"\"\n",
    "    Get names of existing components in the output directory.\n",
    "    \n",
    "    Args:\n",
    "        output_dir (str): Path to components directory\n",
    "        \n",
    "    Returns:\n",
    "        set: Set of component names (without .tsx extension)\n",
    "    \"\"\"\n",
    "    existing_names = set()\n",
    "    if os.path.exists(output_dir):\n",
    "        for file in os.listdir(output_dir):\n",
    "            if file.endswith('.tsx'):\n",
    "                existing_names.add(file[:-4])\n",
    "    return existing_names\n",
    "\n",
    "def print_token_usage(shot_number: int, usage: TokenUsage):\n",
    "    \"\"\"\n",
    "    Print token usage statistics for a generation shot.\n",
    "    \n",
    "    Args:\n",
    "        shot_number (int): Shot number (1 or 2)\n",
    "        usage (TokenUsage): Token usage statistics\n",
    "    \"\"\"\n",
    "    print(f\"\\n  📊 Shot {shot_number} Token Usage:\")\n",
    "    print(f\"    Input tokens: {usage.input_tokens}\")\n",
    "    print(f\"    Output tokens: {usage.output_tokens}\")\n",
    "    print(f\"    Total tokens: {usage.total_tokens}\")\n",
    "\n",
    "def print_total_token_usage(shot1_usage: TokenUsage, shot2_usage: TokenUsage):\n",
    "    \"\"\"\n",
    "    Print total token usage statistics across both shots.\n",
    "    \n",
    "    Args:\n",
    "        shot1_usage (TokenUsage): First shot token usage\n",
    "        shot2_usage (TokenUsage): Second shot token usage\n",
    "    \"\"\"\n",
    "    total_usage = TokenUsage(\n",
    "        input_tokens=shot1_usage.input_tokens + shot2_usage.input_tokens,\n",
    "        output_tokens=shot1_usage.output_tokens + shot2_usage.output_tokens\n",
    "    )\n",
    "    print(\"\\n  📊 Total Token Usage Across 2-Shot Iteration:\")\n",
    "    print(f\"    Total Input Tokens: {total_usage.input_tokens}\")\n",
    "    print(f\"    Total Output Tokens: {total_usage.output_tokens}\")\n",
    "    print(f\"    Total Tokens: {total_usage.total_tokens}\")\n",
    "\n",
    "# Print confirmation\n",
    "print(\"✅ Base functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Component generation functions loaded\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 - Component Generation Core\n",
    "\n",
    "def generate_concept_understanding(client, title: str, prompt: str) -> Tuple[str, TokenUsage]:\n",
    "    \"\"\"\n",
    "    Generate conceptual understanding using Claude API (First shot).\n",
    "    \n",
    "    Args:\n",
    "        client: Anthropic client instance\n",
    "        title: The concept title\n",
    "        prompt: Generated first-shot prompt\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (concept_explanation, token_usage)\n",
    "    \"\"\"\n",
    "    print(f\"\\n  ⌛ Stage 1: Generating conceptual understanding for {title}...\")\n",
    "    \n",
    "    system_prompt_concept = '''\n",
    "    You are a creative expert React developer and Artificial Intelligence professor specialized in educational components for 15 to 18-year-old human students.\n",
    "    Your job is to come up with react components that explain and illustrates concepts in the field of AI and Machine Learning. Do not write any code. Strive for novelty.\n",
    "    '''\n",
    "    \n",
    "    try:\n",
    "        response = client.messages.create(\n",
    "            model=MODEL_NAME,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            temperature=TEMPERATURE,\n",
    "            system=system_prompt_concept,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        \n",
    "        concept_response = response.content[0].text\n",
    "        token_usage = TokenUsage.from_response(response)\n",
    "        print_token_usage(1, token_usage)\n",
    "        print(f\"\\n  💬 Stage 1 response: {concept_response}\")\n",
    "        \n",
    "        return concept_response, token_usage\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error generating concept understanding: {str(e)}\")\n",
    "        return None, TokenUsage()\n",
    "\n",
    "def generate_component_implementation(\n",
    "    client,\n",
    "    title: str,\n",
    "    prompt: str,\n",
    "    concept_response: str\n",
    ") -> Tuple[str, TokenUsage]:\n",
    "    \"\"\"\n",
    "    Generate React component implementation using Claude API (Second shot).\n",
    "    \n",
    "    Args:\n",
    "        client: Anthropic client instance\n",
    "        title: The concept title\n",
    "        prompt: Generated second-shot prompt\n",
    "        concept_response: Response from first shot\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (component_code, token_usage)\n",
    "    \"\"\"\n",
    "    print(f\"\\n  ⌛ Stage 2: Generating component implementation for {title}...\")\n",
    "    \n",
    "    system_prompt_implementation = '''\n",
    "You are a creative expert React developer and AI professor specializing in educational components for 15 to 18-year-old humans. \n",
    "Your components must strictly follow these technical requirements:\n",
    "\n",
    "1. Architecture:\n",
    "- \"use client\" directive at start (first line)\n",
    "- import { useState, useEffect } from \"react\"; as the second line\n",
    "- Only useState and useEffect hooks\n",
    "- Only Tailwind CSS for styling\n",
    "- No external libraries/components\n",
    "- File extension: .tsx\n",
    "- Only Lucide icons for visuals. Limit yourself to these icons: ''' + str(LUCIDEICONS) + '''\n",
    "\n",
    "2. TypeScript Implementation:\n",
    "interface ComponentProps {\n",
    "    // Define if needed, empty interface required\n",
    "}\n",
    "// All state must use explicit types\n",
    "const [state, setState] = useState<StateType>(initialValue);\n",
    "// Event handlers must be typed\n",
    "const handleEvent = (e: React.MouseEvent<HTMLButtonElement>) => {...};\n",
    "// Constants outside component\n",
    "const SCENARIOS: ScenarioType[] = [...];\n",
    "\n",
    "3. Effects & Cleanup:\n",
    "useEffect(() => {\n",
    "    // Effect logic\n",
    "    return () => {\n",
    "    // Cleanup required\n",
    "    };\n",
    "}, [dependencies]);\n",
    "\n",
    "4. Styling Standards:\n",
    "- Only core Tailwind classes\n",
    "- No arbitrary values (e.g., h-[500px])\n",
    "- Transitions: duration-300 to duration-500\n",
    "- Color scheme:\n",
    "    • Blue (#3B82F6) - active/focus\n",
    "    • Gray (#6B7280) - background\n",
    "    • Green (#22C55E) - success\n",
    "\n",
    "5. Code Organization:\n",
    "- Max 200 lines per component\n",
    "- Early returns with type guards\n",
    "- JSDoc component documentation\n",
    "- Proper hooks cleanup\n",
    "- No inline styles\n",
    "- No setTimeout/setInterval (use useEffect)\n",
    "\n",
    "6. Accessibility:\n",
    "- ARIA roles and attributes where needed\n",
    "- Keyboard navigation support\n",
    "- Proper focus management\n",
    "- Color contrast (4.5:1 for text, 3:1 for large text)\n",
    "- Multiple ways to convey information\n",
    "\n",
    "Return only raw TSX code without explanations or markdown.\n",
    "'''\n",
    "\n",
    "    try:\n",
    "        response = client.messages.create(\n",
    "            model=MODEL_NAME,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            temperature=TEMPERATURE,\n",
    "            system=system_prompt_implementation,\n",
    "            messages=[{\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"{prompt}\\n\\nFirst-shot understanding:\\n{concept_response}\"\n",
    "            }]\n",
    "        )\n",
    "        \n",
    "        component_code = response.content[0].text\n",
    "        \n",
    "        # Clean up the code if it's wrapped in markdown\n",
    "        if component_code.startswith('```'):\n",
    "            first_newline = component_code.find('\\n')\n",
    "            if first_newline != -1:\n",
    "                component_code = component_code[first_newline + 1:]\n",
    "            if component_code.strip().endswith('```'):\n",
    "                component_code = component_code.strip()[:-3]\n",
    "        \n",
    "        token_usage = TokenUsage.from_response(response)\n",
    "        print_token_usage(2, token_usage)\n",
    "        \n",
    "        return component_code, token_usage\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error generating component implementation: {str(e)}\")\n",
    "        return None, TokenUsage()\n",
    "\n",
    "def generate_component_with_refinement(\n",
    "    client,\n",
    "    title: str,\n",
    "    summary: str\n",
    ") -> Tuple[str, List[str], TokenUsage, TokenUsage]:\n",
    "    \"\"\"\n",
    "    Two-stage component generation with conceptual understanding and implementation.\n",
    "    \n",
    "    Args:\n",
    "        client: Anthropic client instance\n",
    "        title: Component title for logging\n",
    "        summary: Brief summary of the concept\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (final_component_code, validation_issues, shot1_usage, shot2_usage)\n",
    "    \"\"\"\n",
    "    # Stage 1: Conceptual Understanding\n",
    "    concept_prompt = create_concept_prompt(title, summary)\n",
    "    concept_response, shot1_usage = generate_concept_understanding(\n",
    "        client,\n",
    "        title,\n",
    "        concept_prompt\n",
    "    )\n",
    "    \n",
    "    if not concept_response:\n",
    "        return None, [\"Concept generation failed\"], shot1_usage, TokenUsage()\n",
    "    \n",
    "    # Stage 2: Implementation\n",
    "    implementation_prompt = create_implementation_prompt(\n",
    "        title,\n",
    "        summary,\n",
    "        concept_response\n",
    "    )\n",
    "    \n",
    "    component_code, shot2_usage = generate_component_implementation(\n",
    "        client,\n",
    "        title,\n",
    "        implementation_prompt,\n",
    "        concept_response\n",
    "    )\n",
    "    \n",
    "    if not component_code:\n",
    "        return None, [\"Implementation generation failed\"], shot1_usage, shot2_usage\n",
    "    \n",
    "    # Validate the generated code\n",
    "    issues = validate_component(component_code)\n",
    "    \n",
    "    if not issues:\n",
    "        print(\"  ✅ Component validation passed\")\n",
    "    else:\n",
    "        print(\"\\n  ⚠️ Component validation issues found:\")\n",
    "        for issue in issues:\n",
    "            print(f\"    {issue}\")\n",
    "    \n",
    "    # Print total token usage\n",
    "    print_total_token_usage(shot1_usage, shot2_usage)\n",
    "    \n",
    "    return component_code, issues, shot1_usage, shot2_usage\n",
    "\n",
    "# Print confirmation\n",
    "print(\"✅ Component generation functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Component save function loaded\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 - Component Validation and Fixes\n",
    "\n",
    "def save_tsx_file(\n",
    "    content: str,\n",
    "    md_filename: str,\n",
    "    output_dir: str,\n",
    "    shot1_usage: TokenUsage,\n",
    "    shot2_usage: TokenUsage\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Save the API response as a .tsx file with minimal validation.\n",
    "    \n",
    "    Args:\n",
    "        content (str): The component code to save\n",
    "        md_filename (str): Original markdown filename with .md extension\n",
    "        output_dir (str): Directory to save the TSX file\n",
    "        shot1_usage (TokenUsage): Token usage from first shot\n",
    "        shot2_usage (TokenUsage): Token usage from second shot\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Convert .md to .tsx while preserving exact filename\n",
    "    tsx_filename = md_filename.replace('.md', '.tsx')\n",
    "    filepath = os.path.join(output_dir, tsx_filename)\n",
    "    \n",
    "    # Clean the content\n",
    "    cleaned_content = content\n",
    "    if content.startswith('```'):\n",
    "        first_newline = content.find('\\n')\n",
    "        if first_newline != -1:\n",
    "            cleaned_content = content[first_newline + 1:]\n",
    "        if cleaned_content.strip().endswith('```'):\n",
    "            cleaned_content = cleaned_content.strip()[:-3]\n",
    "    \n",
    "    # Check and ensure \"use client\" directive\n",
    "    cleaned_content = cleaned_content.strip()\n",
    "    if not cleaned_content.startswith('\"use client\"'):\n",
    "        cleaned_content = '\"use client\"\\n\\n' + cleaned_content\n",
    "        print(\"  🔧 Added missing 'use client' directive\")\n",
    "    \n",
    "    # Log token usage\n",
    "    component_name = os.path.splitext(md_filename)[0]\n",
    "    log_token_usage(component_name, shot1_usage, shot2_usage)\n",
    "    update_token_summary()\n",
    "    \n",
    "    # Save the file\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(cleaned_content)\n",
    "    print(f\"  ✓ Saved: {tsx_filename}\")\n",
    "\n",
    "# Print confirmation\n",
    "print(\"✅ Component save function loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 - Main Execution\n",
    "\n",
    "def process_file(client, md_file, metadata):\n",
    "    \"\"\"\n",
    "    Process a single file to generate its component.\n",
    "    \n",
    "    Args:\n",
    "        client: Anthropic client instance\n",
    "        md_file: Path object for the markdown file\n",
    "        metadata: Dictionary containing file metadata\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (success status, shot1_usage, shot2_usage)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"  ⌛ Creating prompts for: {metadata['title']}\")\n",
    "        \n",
    "        # First shot - Conceptual Understanding\n",
    "        concept_prompt = create_concept_prompt(metadata['title'], metadata['summary'])\n",
    "        concept_response, shot1_usage = generate_concept_understanding(\n",
    "            client,\n",
    "            metadata['title'],\n",
    "            concept_prompt\n",
    "        )\n",
    "        \n",
    "        if not concept_response:\n",
    "            print(f\"  ❌ Failed to generate concept understanding for: {md_file.name}\")\n",
    "            return False, TokenUsage(), TokenUsage()\n",
    "            \n",
    "        # Second shot - Implementation\n",
    "        implementation_prompt = create_implementation_prompt(\n",
    "            metadata['title'],\n",
    "            metadata['summary'],\n",
    "            concept_response\n",
    "        )\n",
    "        \n",
    "        component_code, shot2_usage = generate_component_implementation(\n",
    "            client,\n",
    "            metadata['title'],\n",
    "            implementation_prompt,\n",
    "            concept_response\n",
    "        )\n",
    "        \n",
    "        if component_code:\n",
    "            save_tsx_file(\n",
    "                component_code,\n",
    "                md_file.name,\n",
    "                OUTPUT_DIR,\n",
    "                shot1_usage,\n",
    "                shot2_usage\n",
    "            )\n",
    "            return True, shot1_usage, shot2_usage\n",
    "        else:\n",
    "            print(f\"  ❌ Failed to generate component for: {md_file.name}\")\n",
    "            return False, shot1_usage, shot2_usage\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error processing file: {str(e)}\")\n",
    "        return False, TokenUsage(), TokenUsage()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function for the component generator.\"\"\"\n",
    "    print(\"\\n🚀 Starting AI Component Generator...\\n\")\n",
    "    start_time_total = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Initialize Anthropic client\n",
    "        client = anthropic.Client(api_key=os.getenv('ANTHROPIC_API_KEY'))\n",
    "        \n",
    "        # Check directories\n",
    "        print(\"📂 Checking directories...\")\n",
    "        if not os.path.exists(INPUT_DIR):\n",
    "            raise Exception(f\"Input directory not found: {INPUT_DIR}\")\n",
    "        if not os.path.exists(OUTPUT_DIR):\n",
    "            os.makedirs(OUTPUT_DIR)\n",
    "            print(f\"  ✓ Created output directory: {OUTPUT_DIR}\")\n",
    "        \n",
    "        # Get existing component names\n",
    "        print(\"\\n📂 Checking existing components...\")\n",
    "        existing_components = get_existing_component_names(OUTPUT_DIR)\n",
    "        if existing_components:\n",
    "            print(f\"  ✓ Found {len(existing_components)} existing components\")\n",
    "        \n",
    "        # Get list of all .md files that don't have corresponding .tsx files\n",
    "        all_md_files = []\n",
    "        for md_file in Path(INPUT_DIR).glob('*.md'):\n",
    "            if md_file.stem not in existing_components:\n",
    "                all_md_files.append(md_file)\n",
    "        \n",
    "        total_available = len(all_md_files)\n",
    "        print(f\"\\n📁 Found {total_available} unprocessed files\")\n",
    "        \n",
    "        if total_available == 0:\n",
    "            print(\"❌ No new files to process\")\n",
    "            return\n",
    "        \n",
    "        # Select 50 random files (or all if less than 50 available)\n",
    "        num_files = min(1, total_available)\n",
    "        md_files = random.sample(all_md_files, num_files)\n",
    "        \n",
    "        print(f\"\\n🎲 Randomly selected {num_files} files to process\")\n",
    "        \n",
    "        # Track statistics\n",
    "        successful = 0\n",
    "        failed = 0\n",
    "        total_tokens = TokenUsage()\n",
    "        \n",
    "        # Process each file\n",
    "        for index, md_file in enumerate(md_files, 1):\n",
    "            print(f\"\\n📝 Processing file {index}/{num_files}: {md_file.name}\")\n",
    "            start_time_file = time.time()\n",
    "            \n",
    "            try:\n",
    "                print(\"  ⌛ Reading file...\")\n",
    "                with open(md_file, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                \n",
    "                print(\"  ⌛ Extracting metadata...\")\n",
    "                metadata = extract_frontmatter(content)\n",
    "                if not metadata:\n",
    "                    print(\"  ❌ Could not extract metadata\")\n",
    "                    failed += 1\n",
    "                    continue\n",
    "                \n",
    "                # Process the file\n",
    "                success, shot1_usage, shot2_usage = process_file(client, md_file, metadata)\n",
    "                if success:\n",
    "                    successful += 1\n",
    "                    total_tokens.input_tokens += shot1_usage.input_tokens + shot2_usage.input_tokens\n",
    "                    total_tokens.output_tokens += shot1_usage.output_tokens + shot2_usage.output_tokens\n",
    "                else:\n",
    "                    failed += 1\n",
    "                \n",
    "                elapsed_time = time.time() - start_time_file\n",
    "                print(f\"  ⏱️ Time taken: {format_time(elapsed_time)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ Error: {str(e)}\")\n",
    "                failed += 1\n",
    "        \n",
    "        # Print summary\n",
    "        total_time = time.time() - start_time_total\n",
    "        print(\"\\n====== Summary ======\")\n",
    "        print(f\"✅ Successfully processed: {successful}\")\n",
    "        print(f\"❌ Failed: {failed}\")\n",
    "        print(f\"⏱️ Total time: {format_time(total_time)}\")\n",
    "        print(\"\\n📊 Token Usage Summary:\")\n",
    "        print(f\"  Total input tokens: {total_tokens.input_tokens}\")\n",
    "        print(f\"  Total output tokens: {total_tokens.output_tokens}\")\n",
    "        print(f\"  Total tokens: {total_tokens.total_tokens}\")\n",
    "        if successful > 0:\n",
    "            print(f\"  Average tokens per component: {total_tokens.total_tokens / successful:,.0f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Fatal error: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    print(\"\\n✨ Process completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Starting AI Component Generator...\n",
      "\n",
      "📂 Checking directories...\n",
      "\n",
      "📂 Checking existing components...\n",
      "  ✓ Found 93 existing components\n",
      "\n",
      "📁 Found 727 unprocessed files\n",
      "\n",
      "🎲 Randomly selected 1 files to process\n",
      "\n",
      "📝 Processing file 1/1: privileged-instructions.md\n",
      "  ⌛ Reading file...\n",
      "  ⌛ Extracting metadata...\n",
      "  ⌛ Creating prompts for: Privileged Instructions\n",
      "\n",
      "  ⌛ Stage 1: Generating conceptual understanding for Privileged Instructions...\n",
      "\n",
      "  📊 Shot 1 Token Usage:\n",
      "    Input tokens: 327\n",
      "    Output tokens: 579\n",
      "    Total tokens: 906\n",
      "\n",
      "  💬 Stage 1 response: I'll break this down into an engaging explanation using metaphors and then propose an interactive React component concept.\n",
      "\n",
      "METAPHORS & REAL-WORLD EXAMPLES:\n",
      "\n",
      "1. \"The VIP Nightclub\" Metaphor\n",
      "- The nightclub represents the computer system\n",
      "- Regular guests are normal programs/instructions\n",
      "- VIP area represents privileged mode\n",
      "- Bouncers are the CPU's protection mechanism\n",
      "- VIP wristbands represent privilege level flags\n",
      "Maps to: Different privilege levels, access control, protection rings\n",
      "Potential misconception: Might suggest privileges are permanent (like VIP status)\n",
      "\n",
      "2. \"High School Administration\" Metaphor\n",
      "- Students are regular programs\n",
      "- Teachers are user-level administrators\n",
      "- Principal/Admin staff are privileged instructions\n",
      "- School keys/access cards are privilege levels\n",
      "- Security system is protection mechanism\n",
      "Maps to: Hierarchy of permissions, necessity of restrictions\n",
      "Potential misconception: Might oversimplify the dynamic nature of privilege switching\n",
      "\n",
      "REACT COMPONENT CONCEPT: \"The Secure School Simulator\"\n",
      "\n",
      "Interactive Features:\n",
      "1. Visual School Layout\n",
      "- Different colored zones representing privilege levels\n",
      "- Moving characters (programs) with different access levels\n",
      "- Visual feedback when access is denied/granted\n",
      "\n",
      "2. Interactive Elements:\n",
      "- Students can attempt actions as different roles\n",
      "- Real-time feedback on why certain actions are restricted\n",
      "- Privilege escalation requests with explanations\n",
      "- Visual representation of protection rings\n",
      "\n",
      "3. Challenge Mode:\n",
      "- Scenarios where students must decide which privilege level is needed\n",
      "- Security breach scenarios to identify inappropriate access attempts\n",
      "- Score tracking for correct privilege management\n",
      "\n",
      "Learning Verification:\n",
      "- Interactive quizzes embedded in the simulation\n",
      "- Scenario-based problems\n",
      "- \"Explain why\" prompts after access denials\n",
      "\n",
      "Key Learning Objectives:\n",
      "1. Students should understand:\n",
      "- Why privileged instructions exist\n",
      "- The concept of protection rings\n",
      "- When privilege escalation is necessary\n",
      "- Security implications of privilege levels\n",
      "\n",
      "2. Students should be able to:\n",
      "- Identify appropriate privilege levels for different operations\n",
      "- Explain why certain operations require higher privileges\n",
      "- Recognize potential security risks in privilege management\n",
      "\n",
      "Visual Elements:\n",
      "- Color-coded privilege zones (Ring 0-3)\n",
      "- Animated character movements\n",
      "- Access denied/granted animations\n",
      "- Protection ring diagram overlay\n",
      "- Real-time status indicators\n",
      "\n",
      "Interaction Flow:\n",
      "1. Start as regular user\n",
      "2. Attempt various operations\n",
      "3. Request privilege escalation\n",
      "4. Observe protection mechanisms\n",
      "5. Complete challenges\n",
      "6. Receive feedback and explanations\n",
      "\n",
      "This component would make abstract concepts tangible through familiar contexts while maintaining technical accuracy and engaging interaction.\n",
      "\n",
      "  ⌛ Stage 2: Generating component implementation for Privileged Instructions...\n",
      "\n",
      "  📊 Shot 2 Token Usage:\n",
      "    Input tokens: 44467\n",
      "    Output tokens: 1565\n",
      "    Total tokens: 46032\n",
      "  ✓ Saved: privileged-instructions.tsx\n",
      "  ⏱️ Time taken: 0m 38s\n",
      "\n",
      "====== Summary ======\n",
      "✅ Successfully processed: 1\n",
      "❌ Failed: 0\n",
      "⏱️ Total time: 0m 38s\n",
      "\n",
      "📊 Token Usage Summary:\n",
      "  Total input tokens: 44794\n",
      "  Total output tokens: 2144\n",
      "  Total tokens: 0\n",
      "  Average tokens per component: 0\n",
      "\n",
      "✨ Process completed!\n"
     ]
    }
   ],
   "source": [
    "# Cell 6\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
