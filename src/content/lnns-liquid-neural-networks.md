---
category: ARCH, CORE
generality:
- 0.6
- 0.59
- 0.58
- 0.57
- 0.56
- 0.55
- 0.54
slug: lnns
summary: Innovative ML models designed to learn new tasks without impacting previously learnt tasks, thus preventing catastrophic forgetting.
title: LNNs (Liquid Neural Networks)
---

Liquid Neural Networks (Liquid NNs) signify a recent advancement within the field of AI, specifically addressing the problem of 'catastrophic forgetting' in neural networks. Unlike traditional neural networks that might overwrite previously learned information when trained on new datasets, Liquid NNs adapt to new tasks while retaining knowledge from previous tasks. This is achieved by maintaining a pool of diverse models and selecting subsets which are consolidated for each task, allowing for a fluid shift in skillsets, akin to a liquid's adaptability to changing containers.

Historical Overview: Liquid NNs are a relatively new concept, emerging in the arena of AI research during the late 2010s. This concept quickly gained popularity addressing the long-standing issue of catastrophic forgetting in AI models, thus revolutionizing continual learning.

Key contributors: OpenAI, the pioneering AI research organization, has been instrumental in developing and promoting the concept of Liquid NNs, pushing the boundaries of continual learning in AI. Researchers such as Tom Schaul and Gido Van de Ven have also published influential work in this domain.