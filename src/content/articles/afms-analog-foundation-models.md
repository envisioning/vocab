---
title: AFMs (Analog Foundation Models)
summary: Large-scale pretrained models architected or deployed to run primarily on analog computation substrates (e.g., memristive crossbars, photonic processors, or neuromorphic circuits) to exploit orders-of-magnitude improvements in energy and latency compared with conventional digital accelerators.
slug: afms-analog-foundation-models
---

Analog Foundation Models are foundation-scale AI models adapted for, or natively designed to exploit, analog compute substrates; they integrate hardware-aware architectures and training methods to tolerate device nonidealities (noise, drift, limited precision, nonlinearity) and to leverage in-memory or photonic parallelism for highly efficient inference and increasingly for in‑situ training. From a systems perspective AFMs require co-design across device physics, circuit topology, compiler/mapping algorithms, and ML training recipes: common approaches include hybrid digital‑analog splits, analog-aware quantization and noise-injection during training, closed‑form or learned calibration layers, and architecture modifications (sparsity, activation shapes, and normalization schemes) that are robust to analog variability. The theoretical and practical significance is twofold: (1) AFMs extend the foundation‑model paradigm (large, generalizable pretrained models) to domains constrained by power, latency, or form factor (edge/embedded systems, data‑center energy caps, mobile robotics), and (2) they drive new research directions in robust optimization, device‑aware model compression, and co-designed toolchains that reframe limits of model scale versus hardware cost.

First usages of the phrase emerged in the research and industry discourse around 2022–2024 as demonstrations of memristive, photonic, and neuromorphic accelerators began to host increasingly large pretrained models; the term gained wider traction in 2024–2025 when prototype deployments and benchmarking (showing substantial energy and latency benefits for inference and select on‑chip fine‑tuning) made analog approaches a credible complement to purely digital ML accelerators.

Key contributors are multidisciplinary: academic pioneers in neuromorphic and in‑memory computing (e.g., Kwabena Boahen and others in neuromorphic engineering), memristor and resistive crossbar researchers (e.g., R. Stanley Williams, Dmitri Strukov and associated groups), photonic neural‑network researchers (e.g., David A. B. Miller and several photonics labs), and engineering teams at startups and companies pushing analog accelerators into ML workloads (examples include Mythic for memristive inference, Lightmatter and LightOn for photonic/optical acceleration, and groups at Intel, IBM and various university labs developing toolchains and device‑aware training methodologies).