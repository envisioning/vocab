---
title: Hopfield networks
summary: Recurrent, energy-based neural networks that implement content-addressable associative memory by converging to stored pattern attractors.
slug: hopfield-networks
---

Hopfield networks are fully connected recurrent (symmetric-weight) neural systems whose dynamics minimize a defined energy (Lyapunov) function so that the network evolves to fixed-point attractors that represent stored patterns; using Hebbian-style weight setting they perform pattern completion and error correction from partial or noisy cues. In technical terms, the classical binary Hopfield model uses N binary units with symmetric weights and asynchronous (or synchronous) updates governed by an energy function; stable fixed points correspond to local minima of that energy, and retrieval is analysis of attractor basins. Extensions include continuous-valued neurons, stochastic update rules (Glauber dynamics), and dense associative memories that increase capacity and alter basin geometry. The model has deep connections to statistical physics (Ising spin-glass analyses), a well-characterized storage capacity (≈0.138N for uncorrelated random binary patterns in the original model), and practical adaptations for optimization (e.g., formulations for combinatorial problems). Limitations such as spurious minima, limited scalability, and sensitivity to pattern correlations motivated theoretical refinements and modern reinterpretations linking Hopfield-style energy landscapes to contemporary energy-based ML models and attention mechanisms.

Introduced in 1982 (John J. Hopfield) and popularized through mid-1980s work—both theoretical analyses and applications such as Hopfield & Tank’s work on combinatorial optimization—the concept saw a resurgence of interest in the late 2010s/early 2020s as researchers connected Hopfield dynamics to modern attention and dense associative-memory formulations in ML.

Key contributors include John J. Hopfield (foundational 1982 paper), David H. Tank (applications to optimization), Daniel Amit, Hanoch Gutfreund and Haim Sompolinsky (statistical-physics analyses of capacity and retrieval), and later researchers who developed continuous, stochastic, and dense variants (e.g., Krotov & Hopfield) and those who tied Hopfield models to modern attention/energy-based methods (e.g., Ramsauer et al. and other recent work on “modern Hopfield networks”).