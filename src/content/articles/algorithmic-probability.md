---
generality:
- 0.87
- 0.865
- 0.86
- 0.855
- 0.85
- 0.845
- 0.84
slug: algorithmic-probability
summary: Quantifies the likelihood that a random program will produce a specific output
  on a universal Turing machine, forming a core component of algorithmic information
  theory.
title: Algorithmic Probability
year_origin: 1960
---

Algorithmic probability, also known as Solomonoff probability, measures the probability that a random program, when executed on a universal Turing machine, will produce a specific output. This concept is foundational in algorithmic information theory and is closely related to Kolmogorov complexity, which deals with the shortest possible description of an object. Algorithmic probability provides a framework for predicting future data based on past observations by weighting possible continuations of the data by their simplicity (i.e., the length of the shortest program that can generate them). This probabilistic approach is significant in fields such as machine learning, where it underpins methods for inductive inference and is used in the development of prediction algorithms and data compression techniques.

The concept of algorithmic probability was introduced by Ray Solomonoff in 1960 as part of his work on algorithmic information theory. It gained popularity in the subsequent decades as foundational theories in computational learning and artificial intelligence were developed and refined.

Ray Solomonoff is the primary figure associated with the development of algorithmic probability, having introduced the concept and contributed extensively to its theoretical underpinnings. His work laid the groundwork for later developments in AI and machine learning, influencing notable researchers in these fields.