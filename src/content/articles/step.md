---
title: Step
summary: A single iteration in the process of updating AI model parameters within an optimization algorithm.
slug: step
---

In AI, particularly in optimization processes like those used for training neural networks, a "step" refers to the execution of a single update to the model's parameters. During training, algorithms like gradient descent utilize steps to iteratively minimize the loss function and improve model accuracy. Each step involves calculating the gradient of the loss function with respect to the parameters, adjusting those parameters by a small amount proportional to this gradient, and then re-evaluating the loss. The size of these adjustments is determined by a hyperparameter known as the learning rate. Steps are fundamental to the convergence of the model towards an optimal set of parameters, with their efficiency influencing the speed and success of the training process.

The term "step" in the context of optimization has been used since the early development of iterative computational methods in the 20th century, but gained prominence in the AI field with the popularization of ML techniques in the 1980s and 1990s, particularly with the renewed interest in neural networks.

Key contributors to the concept of steps in optimization algorithms include researchers like David Rumelhart, Geoffrey Hinton, and Ronald J. Williams, whose work on backpropagation popularized the use of small, iterative updates in training neural networks.