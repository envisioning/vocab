---
category: MATH, CORE
generality:
- 0.88
- 0.865
- 0.85
- 0.835
- 0.82
- 0.805
- 0.79
slug: scaling-laws
summary: Mathematical relationships that describe how the performance of machine learning models, particularly deep learning models, improves as their size, the amount of data, or computational resources increases.
title: Scaling Laws
---

**Detailed Explanation:** Scaling laws provide a framework to predict the behavior of AI models as they grow in size, data, or compute power. These laws suggest that by increasing model parameters, training data, or computational resources, AI systems can achieve better performance on tasks like language modeling, image recognition, and other domains. Specifically, scaling laws reveal power-law relationships where performance improves predictably with increased scale, but with diminishing returns at very large scales. This has profound implications for the design of future AI systems, as it emphasizes the importance of efficient resource use and motivates the development of architectures that can better exploit scale. Such laws also inform cost-performance trade-offs and long-term AI research strategies by showing the limits and potential of continued scaling.

**Historical Overview:** The concept of scaling laws became prominent in the context of deep learning around 2018-2020, following groundbreaking work on large language models like OpenAI's GPT series and other massive neural networks. Researchers like Kaplan et al. (2020) at OpenAI formally documented scaling behaviors, bringing the idea into the mainstream of AI research.

**Key Contributors:** Key contributors to the development of scaling laws include Jared Kaplan and his collaborators at OpenAI, who published a seminal paper on scaling laws for neural language models. Their research quantified how models' performance scales with increases in model size, data, and compute, providing an empirical framework that has shaped the development of large-scale AI systems.