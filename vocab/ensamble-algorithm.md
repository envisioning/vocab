---
category: CORE, ARCH
generality:
- 0.89
- 0.88
- 0.87
- 0.86
- 0.85
- 0.84
- 0.83
slug: ensamble-algorithm
summary: Combines multiple machine learning models to improve overall performance
  by reducing bias, variance, or noise.
title: Ensamble Algorithm
---

- **Detailed Explanation**: Ensemble algorithms are designed to leverage the collective strengths of several models, often of different types, to improve predictive accuracy and robustness. This approach helps address limitations that individual models might have, such as overfitting, underfitting, or sensitivity to noise. Common techniques include **bagging**, which builds multiple versions of a model on different subsets of the data (e.g., Random Forest), and **boosting**, which sequentially trains models to correct errors from previous ones (e.g., AdaBoost, Gradient Boosting). By aggregating the predictions of multiple models, ensemble methods tend to outperform single-model approaches, especially in complex, high-dimensional datasets or noisy environments.
    
- **Historical Overview**: The concept of ensemble methods dates back to the early 1990s, gaining significant attention with the development of popular algorithms like Bagging (1994) by Leo Breiman and Boosting (1997) by Yoav Freund and Robert Schapire. These techniques started gaining broader popularity in the 2000s, with Random Forests and boosting algorithms becoming standard tools in machine learning.
    
- **Key Contributors**: Leo Breiman is credited with introducing Bagging in 1994, a fundamental ensemble method. Robert Schapire and Yoav Freund made critical contributions with their development of Boosting algorithms in the mid-1990s, particularly AdaBoost, which significantly advanced the field of ensemble learning.